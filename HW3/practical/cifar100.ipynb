{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JjB0dTd54WG"
      },
      "source": [
        "# Homework 3: optimization of a CNN model\n",
        "The task of this homework is to optimize a CNN model for the CIFAR-100. You are free to define the architecture of the model, and the training procedure. The only contraints are:\n",
        "- It must be a `torch.nn.Module` object\n",
        "- The number of trained parameters must be less than 1 million\n",
        "- The test dataset must not be used for any step of training. It is better if don't even import it.\n",
        "- The final training notebook should run on Google Colab within a maximum 1 hour approximately.\n",
        "\n",
        "For the grading, you must use the `evaluate` function defined below. It takes a model as input, and returns the test accuracy as output.\n",
        "\n",
        "As a guideline, you are expected to **discuss** and motivate your choices regarding:\n",
        "- Model architecture\n",
        "- Hyperparameters (learning rate, batch size, etc)\n",
        "- Regularization methods\n",
        "- Optimizer\n",
        "- Validation scheme\n",
        "\n",
        "A code without any explanation of the choices will not be accepted. Test accuracy is not the only measure of success for this homework.\n",
        "\n",
        "Remember that most of the train process is randomized, store your model's weights after training and load it before the evaluation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfNvNIxz54WI"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5Mgthn54WJ"
      },
      "source": [
        "### Loading packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWHU_pQG54WJ",
        "outputId": "c74a6a20-870f-49dd-be68-461f6bea36ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from evaluate import evaluate\n",
        "\n",
        "# Import the best device available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# load the data\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXBRG6qX54WL"
      },
      "source": [
        "### Example of a simple CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_zvq0_l54WL",
        "outputId": "73895251-7797-4b2b-cd92-63566ef2320f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:  556708\n"
          ]
        }
      ],
      "source": [
        "class TinyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = torch.nn.Linear(8*8*64, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model parameters: \", sum(p.numel() for p in TinyNet().parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72vikBOB54WL"
      },
      "source": [
        "### Example of basic training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owDg_SS654WL",
        "outputId": "f27fc6eb-9adf-4dc7-cf81-7b743f9e0ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.5914\n",
            "Epoch [2/10], Loss: 4.5974\n",
            "Epoch [3/10], Loss: 4.6317\n",
            "Epoch [4/10], Loss: 4.6144\n",
            "Epoch [5/10], Loss: 4.5923\n",
            "Epoch [6/10], Loss: 4.5894\n",
            "Epoch [7/10], Loss: 4.5210\n",
            "Epoch [8/10], Loss: 4.4610\n",
            "Epoch [9/10], Loss: 4.4117\n",
            "Epoch [10/10], Loss: 4.3461\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = TinyNet()\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(10):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE9zuete54WM",
        "outputId": "dfff6f31-643c-493e-d26f-a10d232f61a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 556708 parameters\n",
            "\u001b[1m\u001b[91mAccuracy on the test set: 6.02%\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# save the model on a file\n",
        "torch.save(model.state_dict(), 'tiny_net.pt')\n",
        "\n",
        "loaded_model = TinyNet()\n",
        "loaded_model.load_state_dict(torch.load('tiny_net.pt', weights_only=True))\n",
        "evaluate(loaded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH6RSfuA54WM"
      },
      "source": [
        "- Res net\n",
        "- bottleneck building block for deeper nets with fast training\n",
        "- idenitity shortcut\n",
        "- scheduler for learning rate. study if a plateau is present and in case reduce the lr at plateau\n",
        "- weight initialization using kaming he initialization (works better than xavier)\n",
        "- regularization using weight decay: no dropout becauase when using BN it can be avoided\n",
        "- optimizer: sdg or adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ICDeUCbnFXWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from training_utils import *"
      ],
      "metadata": {
        "id": "ygyDOpRMFocS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = None\n",
        "if torch.cuda.is_available():\n",
        "    # Requires NVIDIA GPU with CUDA installed\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "elif torch.mps.is_available():\n",
        "    # Requires Apple computer with M1 or later chip\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "else:\n",
        "    # Not recommended, because it's slow. Move to Google Colab!\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7gT3AxaHPTf",
        "outputId": "5c477617-deda-47b1-c167-28ad22d7952b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# load the train dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform)\n",
        "\n",
        "# Split the dataset into 40k-10k samples for training-validation.\n",
        "from torch.utils.data import random_split\n",
        "train_dataset,  valid_dataset = random_split(\n",
        "    train_dataset,\n",
        "    lengths=[40000, 10000],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyManlvVSKkR",
        "outputId": "e2084702-f187-4b2f-cc83-78dbc520ca6a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I define a fit function as the one used in the tp"
      ],
      "metadata": {
        "id": "Sso9U7UtFlAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    scheduler_lr: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
        "    val_dataloader: Optional[DataLoader] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    the fit method simply calls the train_epoch() method for a\n",
        "    specified number of epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # keep track of the losses in order to visualize them later\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        train_loss = train_epoch(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        # Validate\n",
        "        if val_dataloader is not None:\n",
        "            val_loss, val_accuracy = predict(\n",
        "                model=model, test_dataloader=val_dataloader, device=device, verbose=False\n",
        "            )\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            print(\n",
        "                f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_accuracy:.0f}%\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}\")\n",
        "        # LR scheduler\n",
        "        if scheduler_lr is not None:\n",
        "            scheduler_lr.step(metrics=val_loss)\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "yC91-AavFV9g"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture I choose is a ResNet. ResNets as we have seen in class are very good network to perform image classification tasks.\n",
        "The one I choose is a residual block ResNEt with a skip connection.\n",
        " Skip connection is important because it allows to have deep networks, which offer better performance, without the problem of the vanishing gradient."
      ],
      "metadata": {
        "id": "0JksOCq_GkI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I define the residual block of the ResNet.\n",
        "My block is a 3-layer block with a bottleneck. I choose this structure because it allows to hava e deep network but still with manageble training times.\n",
        "Each layer is made up of a convolution, a batch normalization and a ReLu used as activation function, in this order.\n",
        "The three covolutions used are the following:\n",
        "- 1x1 convolution layer to reduce dimensions\n",
        "- 3x3 (bottleneck) convolution layer on the reduced dimension\n",
        "- 1x1 convolution to restore the dimension"
      ],
      "metadata": {
        "id": "8kkDLmfeGH-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_planes,\n",
        "            out_channels=planes,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=planes,\n",
        "            out_channels=planes,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Skip connection to match dimensions when necessary\n",
        "        self.skip = nn.Sequential()\n",
        "        if stride > 1 or in_planes != planes:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_planes,\n",
        "                    out_channels=planes,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.skip(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "M-4bHASHHiP3"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I write the network."
      ],
      "metadata": {
        "id": "N7c-RqxJMUzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 32  # Initial number of filters\n",
        "\n",
        "        # First layer: 3x3 Convolution\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Residual layers\n",
        "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
        "\n",
        "        # Global Average Pooling and Fully Connected Layer\n",
        "        self.linear = nn.Linear(128, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                torch.nn.init.constant_(m.weight, 1)\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size(3))  # Global Average Pooling\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "print(\"Model parameters: \", sum(p.numel() for p in ResNet(block=ResidualBlock, num_blocks=[4,3,3]).parameters()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqM6MWkfMaOm",
        "outputId": "7cacc237-b2af-495e-e9ec-9629eedc0491"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:  1103146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ResNet(block=ResidualBlock, num_blocks=[13, 12, 12]).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "scheduler_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n",
        "\n",
        "train_losses, valid_losses, valid_accs =   fit(\n",
        "        model,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 100,\n",
        "        device = DEVICE,\n",
        "        val_dataloader = valid_dataloader,\n",
        "        scheduler_lr = scheduler_lr\n",
        "    )\n",
        "plot_loss( train_losses )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "m0MkwLvkRj02",
        "outputId": "d6df5c77-3218-46c6-e6d8-47105158a2f8"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-0e97f6ae44e1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_losses, valid_losses, valid_accs =   fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-2ef9c0980cb8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dataloader, optimizer, epochs, device, scheduler_lr, val_dataloader)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/training_utils.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# compute the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# perform the gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}