{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "# Part 1: Expected Prediction Risk (EPR) \n",
    "\n",
    "The Expected Prediction Risk (EPR) is a fundamental concept that helps us measure how well our predictions match reality, accounting for all possible scenarios weighted by their probability of occurrence. Consider a supervised learning problem with a dataset $(x_i, y_i)_{i=1}^N$ where $x_i \\in \\mathbb{R}^d$ and $y_i$ are the labels (being them real numbers or categorical). We assume that the data is generated by a joint distribution, with density function $p(x,y)$. We fit a model $f(x)$ to this dataset. \n",
    "\n",
    "Given a measure of risk $L(y, f(x))$, the Expected Prediction Risk (EPR) is defined as:\n",
    "\n",
    "$$\\text{EPR}[f] = \\mathbb{E}_{X,Y}[L(Y, f(X))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us consider the regression case.\n",
    "\n",
    "Consider a dataset $\\mathcal{D}_{regr} = (x_i, y_i)_{i=1}^N$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the class that in regression cases, the squared loss is a natural choice. Consider the squared loss function $L(y, f(x)) = (y - f(x))^2$. We define the Expected Prediction Risk (EPR) as:\n",
    "\n",
    "$$\\text{EPR}[f] = \\mathbb{E}_{X,Y}[(Y - f(X))^2]$$\n",
    "\n",
    "### **Q1**: Simplify the expression of the EPR(f) to obtain that the EPR is minimized by the conditional mean of Y given X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider now a categorical dataset. In order to highlight the difference with the regression case, we will use a different notation to highlight the difference: $\\mathcal{D}_{class} = (x_i, c_i)_{i=1}^N$ where $x_i \\in \\mathbb{R}^d$ and $c_i \\in \\{1, \\ldots, K\\}$ is the true class of the $i$-th sample. We fit a model $\\hat{c}(x){\\in \\{1, \\ldots, K\\}}$ to this dataset.\n",
    "\n",
    "### **Q2** Write the expression of the EPR for the classification case simplified as much as possible for general risk functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Q3** Find the optimal classifier for 0-1 loss: $L(c, \\hat{c}) = \\mathbb{I}(c \\neq \\hat{c})$ known as the Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply these concepts to Gaussian Mixture Models. Let us fix the number of components $K=3$ and consider the following model for the joint distribution $p(x,c)$:\n",
    "\n",
    "$$p(x|c_k) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)$$\n",
    "$$p(c_k) = \\pi_k$$\n",
    "where $\\pi_k$ are the probabilities of the different classes, $\\mu_k$ are the means and $\\Sigma_k$ are the covariance matrices of the $k$-th component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4** Write a python function that generates samples from this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q5** Take now 1000 samples from this model and plot them using matplotlib using the following choice for $\\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^3$: \n",
    "\n",
    "$$\\pi = [0.3, 0.4, 0.3]$$\n",
    "\n",
    "$$\\mu_1 = \\left[\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right], \\mu_2 = \\left[\\begin{array}{c} 3 \\\\ 0 \\end{array}\\right], \\mu_3 = \\left[\\begin{array}{c} 0 \\\\ 3 \\end{array}\\right]$$\n",
    "\n",
    "$$\\Sigma_1 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right], \\Sigma_2 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right], \\Sigma_3 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the decision boundary between two classes $c_i$ and $c_j$ as the set of points where the posterior probability of the two classes is equal. Write the expression of the decision boundary for this model. \n",
    "\n",
    "### **Q6** Compute the posterior probabilities for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q7A** Compute analytically the decision boundary between class 1 and class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q7B** Plot the optimal decision boundary for this model using the above calcultations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q8** Compute the error of the Bayes classifier on the dataset in Q5 as measured by the misclassification rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the case where the covariance matrices are different. Take the following choice for $\\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^3$: \n",
    "\n",
    "$$\\pi = [\\pi_1=0.3, \\pi_2=0.4, \\pi_3=0.3]$$\n",
    "\n",
    "$$\\mu_1 = \\left[\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right], \\mu_2 = \\left[\\begin{array}{c} 3 \\\\ 0 \\end{array}\\right], \\mu_3 = \\left[\\begin{array}{c} 0 \\\\ 3 \\end{array}\\right]$$\n",
    "\n",
    "$$\\Sigma_1 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right], \\Sigma_2 = \\left[\\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\end{array}\\right], \\Sigma_3 = \\left[\\begin{array}{cc} 1 & 0.5 \\\\ 0.5 & 1 \\end{array}\\right]$$\n",
    "\n",
    "\n",
    "### **Q9** What is the decision boundary between class 1 and class 2 in this case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q10** Plot the decision boundary for this model and discuss the difference with the previous case (if any).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q11** Sample 1000 points from the heterogeneous model defined after Q8 and plot them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q12** Compute the error of the Bayes classifier on this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
